---
title: "Homework 6"
author: "Yiming Wang"
date: "Due @ 5pm on December 1, 2017"
output:
  html_document:
    df_print: paged
header-includes:
- \usepackage{bm}
- \newcommand{\Real}{\mathbb{R}}
- \newcommand{\prox}{\operatorname{prox}}
- \newcommand{\dom}{{\bf dom}\,}
- \newcommand{\Tra}{^{\sf T}} % Transpose
- \newcommand{\Inv}{^{-1}} % Inverse
- \def\vec{\mathop{\rm vec}\nolimits}
- \newcommand{\diag}{\mathop{\rm diag}\nolimits}
- \newcommand{\tr}{\operatorname{tr}} % Trace
- \newcommand{\epi}{\operatorname{epi}} % epigraph
- \newcommand{\V}[1]{{\bm{\mathbf{\MakeLowercase{#1}}}}} % vector
- \newcommand{\VE}[2]{\MakeLowercase{#1}_{#2}} % vector element
- \newcommand{\Vn}[2]{\V{#1}^{(#2)}} % n-th vector
- \newcommand{\Vtilde}[1]{{\bm{\tilde \mathbf{\MakeLowercase{#1}}}}} % vector
- \newcommand{\Vhat}[1]{{\bm{\hat \mathbf{\MakeLowercase{#1}}}}} % vector
- \newcommand{\VtildeE}[2]{\tilde{\MakeLowercase{#1}}_{#2}} % vector element
- \newcommand{\M}[1]{{\bm{\mathbf{\MakeUppercase{#1}}}}} % matrix
- \newcommand{\ME}[2]{\MakeLowercase{#1}_{#2}} % matrix element
- \newcommand{\Mtilde}[1]{{\bm{\tilde \mathbf{\MakeUppercase{#1}}}}} % matrix
- \newcommand{\Mbar}[1]{{\bm{\bar \mathbf{\MakeUppercase{#1}}}}} % matrix
- \newcommand{\Mn}[2]{\M{#1}^{(#2)}} % n-th matrix
---

**Part 1.** You will implement matrix completion algorithms. We first set some notation: $\M{X} \in \Real^{n \times p}$ and $\Omega \subset \{1, \ldots, n\} \times \{1, \ldots, p\}$. We seek the matrix $\M{U} \in \Real^{n \times p}$ that minimizes the following objective function
$$
\ell(\M{U}) = \frac{1}{2}\lVert \mathcal{P}_\Omega(\M{X}) - \mathcal{P}_\Omega(\M{U}) \rVert_{\text{F}}^2 + \lambda \lVert \M{U} \rVert_*,
$$
where $\lVert \M{U} \rVert_*$ is the nuclear norm of $\M{U}, \lambda$ is a non-negative regularization parameter. The projection operator $\mathcal{P}_\Omega$ acts as a mask

$$
[\mathcal{P}_\Omega(\M{Z})]_{ij} = \begin{cases}
\ME{z}{ij} & \text{if $(i,j) \in \Omega$} \\
0 & \text{otherwise.}
\end{cases}
$$

1. Prove that the function $\ell(\M{U})$ is convex.

By triangle inequality, $\lVert \mathcal{P}_\Omega(\M{X}) - \mathcal{P}_\Omega(\M{U}) \rVert_{\text{F}}^2$ and $\lVert \M{U} \rVert_*$ are convex. Since $f(x)=\frac{1}{2}x^2$ and $g(x)=\lambda x$ are nondecreasing, $\ell(\M{U})$ is convex.

2. Derive the gradient of $\frac{1}{2}\lVert \mathcal{P}_\Omega(\M{X}) - \mathcal{P}_\Omega(\M{U}) \rVert_{\text{F}}^2$ and prove that it is a 1-Lipschitz mapping.

$$
\nabla\frac{1}{2}\lVert \mathcal{P}_\Omega(\M{X}) - \mathcal{P}_\Omega(\M{U}) \rVert_{\text{F}}^2=\mathcal{P}_\Omega(\M{X}) - \mathcal{P}_\Omega(\M{U})
$$
let $g(\M{U})=\frac{1}{2}\lVert \mathcal{P}_\Omega(\M{X}) - \mathcal{P}_\Omega(\M{U}) \rVert_{\text{F}}^2$, we need to prove
$$
\lVert \nabla g(\M{U})-\nabla g(\M{Q})\rVert_2\leq\lVert\M{U}-\M{Q}\rVert_2
$$

And we have $\lVert \nabla g(\M{U})-\nabla g(\M{Q})\rVert_2=\lVert\mathcal{P}_\Omega(\M{X}) - \mathcal{P}_\Omega(\M{U})\rVert_2=\underset{\lVert \V{x}\rVert=1 }{sup}\lVert\mathcal{P}_\Omega(\M{U}-\M{Q})\V{x}\rVert_2=\underset{\lVert \V{x}\rVert=1 }{sup}(\V{x}^T\mathcal{P}_\Omega(\M{U-Q})^T\mathcal{P}_\Omega(\M{U-Q})\V{x})\leq\underset{\lVert \V{x}\rVert=1 }{sup}(\V{x}^T(\M{U-Q})^T(\M{U-Q})\V{x})=\lVert\M{U-Q}\rVert_2$

Therefore $\lVert \nabla g(\M{U})-\nabla g(\M{Q})\rVert_2\leq\lVert\M{U}-\M{Q}\rVert_2$


3. What is the computational complexity of computing a proximal-gradient step:

$$
\M{U}^+ = \prox_{t\lambda\lVert \cdot \rVert_*}\left ( \M{U} - t \nabla \left\{\frac{1}{2}\lVert \mathcal{P}_\Omega(\M{X}) - \mathcal{P}_\Omega(\M{U}) \rVert_{\text{F}}^2 \right \}\right).
$$
Answer:
$$
\M{U}^+=\prox_{t\lambda\lVert \cdot \rVert_*}\left ( \M{U} - t (\mathcal{P}_\Omega(\M{X}) - \mathcal{P}_\Omega(\M{U}))\right)
$$


First,to get $\nabla \left\{\frac{1}{2}\lVert \mathcal{P}_\Omega(\M{X}) - \mathcal{P}_\Omega(\M{U}) \rVert_{\text{F}}^2 \right\} = \mathcal{P}_\Omega(\M{U}) - \mathcal{P}_\Omega(\M{X})$, it takes $\mathcal{O}(np)$ to calculate the $\M{Y} = \M{U} - t \nabla \left\{\frac{1}{2}\lVert \mathcal{P}_\Omega(\M{X}) - \mathcal{P}_\Omega(\M{U}) \rVert_{\text{F}}^2 \right\}$. Then, it takes $\mathcal{O}(min\{np^2,n^2p\})$ to calculate the SVD of $\M{Y}$. Then, it takes $\mathcal{O}(min\{n, p\})$ to calculate the soft-thresholding of the singular values. Then, it takes $\mathcal{O}(min\{np^2,n^2p\})$ to get back to the new iteration of $\M{U}$ through SVD. Therefore, in total, the computational complexity is $\mathcal{O}(min\{np^2,n^2p\})$.

4. Prove that for all $\lambda \geq \lambda_{\max} = \lVert \mathcal{P}_{\Omega}(\M{X}) \rVert_2$, the all zeros matrix is a global minimizer of $\ell(\M{U})$. You may use the fact that $\partial \lVert \M{0} \rVert_* = \{ \M{V} : \lVert \M{V} \rVert_2 \leq 1\}$.

If $\frac{1}{\lambda} \lVert\mathcal{P}_{\Omega}(\M{X})  \rVert_2 \leq 1$, since it is equivalent to $\frac{1}{\lambda}\mathcal{P}_{\Omega}(\M{X})\in \{ \M{V} : \lVert \M{V} \rVert_2 \leq 1\}$ through writing it as the form of set, we have $-\nabla g( \M{0})\in \partial \lambda \lVert \M{0} \rVert_*$. The former statement is also equivalent to $0 \in \partial \ell(\M{0})$ since $\partial \lVert \M{0} \rVert_* = \{ \M{V} : \lVert \M{V} \rVert_2 \leq 1\}$ and therefore we have that all zeros matrix is a global minimizer of $\ell(\M{U})$.


5. What is the computational complexity of a single soft-impute update? How does this compare to the computational complexity of a single proximal-gradient step?


First, it taks $\mathcal{np}$ flops to calculate the $\M{Y}=\mathcal{P}_{\Omega^c}(\M{U}^{(k-1)})+\mathcal{P}_\Omega(\M{X})$. Then, the rest steps take same flops as the steps in Q3 since they have the same manipulation on the matrix. Therefore, in total, the computational complexity is $\mathcal{O}(min\{np^2,n^2p\})$, which is the same as proximal gradient step. The reason for this equivalence is that the SVD steps are commonly used in both methods and therefore overwhelms the computational complexity.


\newpage

**Part 2.** MM algorithm and proximal gradient algorithm for matrix completion.

Please complete the following steps.

**Step 1:** Write a function "softthreshold" that implements the softthreshold mapping.

```{r, echo=TRUE}
#' Softthreshold operator
#' 
#' @param x input vector
#' @param lambda threshold
#' @export
#' @examples
#' softthreshold(seq(-10, 10, length.out=10), 1)
softthreshold <- function(x, lambda) {
  temp=abs(x)-lambda;
  return(ifelse(temp>0,temp,-temp)*sign(x));
}
```

**Step 2:** Write a function "prox_nuc" that computes the proximal mapping of $\gamma\lVert \cdot \rVert_*$.

```{r, echo=TRUE}
#' Proximal mapping of the scaled nuclear norm
#' 
#' @param X input matrix
#' @param gamma scale parameter
#' @export
#' @examples
#' set.seed(12345)
#' n <- 10; p <- 20
#' X <- matrix(rnorm(n*p), n, p)
#' Y <- prox_nuc(X, 1)
prox_nuc <- function(X, gamma) {
 decomposition=svd(X);
 new_d=softthreshold(decomposition$d,gamma);
 X_map=decomposition$u%*%diag(new_d)%*%t(decomposition$v)
 return(X_map);
}
```

**Step 3:** Write a function "project_omega" that projects a matrix onto an observation index set $\Omega$. Omega consists of indices that are in column major order.

```{r, echo=TRUE}
#' Project matrix onto observed index set
#' 
#' @param X input matrix
#' @param Omega index set
#' @export
#' @examples
#' set.seed(12345)
#' n <- 10; p <- 20
#' X <- matrix(rnorm(n*p), n, p)
#' numel <- n*p
#' Omega <- sample(1:numel, floor(0.5*numel), replace=FALSE)
#' PX <- project_omega(X, Omega)
project_omega <- function(X, Omega) {
  p=ncol(X);n=nrow(X);
  res=Omega%%n;int=Omega%/%n;
  row=ifelse(res==0,n,res);
  col=ifelse(res==0,int,int+1);
  Result=matrix(0,nrow(X),ncol(X));
  for(i in 1:length(Omega))
  Result[row[i],col[i]]=X[row[i],col[i]];
  return(Result);
}
```

\newpage

**Step 4:** Write a function "prox_step" that computes a single proximal gradient step.

```{r, echo=TRUE}
#' Proximal-Gradient Step
#' 
#' @param X input matrix
#' @param U current guess
#' @param Omega index set
#' @param lambda regularization parameter
#' @param t step size
#' @export
#' @examples
#' set.seed(12345)
#' n <- 10; p <- 20
#' X <- matrix(rnorm(n*p), n, p)
#' numel <- n*p
#' Omega <- sample(1:numel, floor(0.5*numel), replace=FALSE)
#' U <- matrix(rnorm(n*p), n, p)
#' U[Omega] <- X[Omega]
#' lambda <- 1
#' t <- 1
#' U1 <- prox_step(X, U, Omega, lambda, t)
prox_step <- function(X, U, Omega, lambda, t) {
  #X_half=X-(project_omega(X,Omega)-project_omega(U,Omega))*t;
  X_half=U-t*project_omega(U-X,Omega);
  out=prox_nuc(X_half,lambda)
  return(out);
}
```

**Step 5:** Write a function "matrix_completion_loss" that computes the objective function $\ell(\M{U})$.

```{r, echo=TRUE}
#' Matrix Completion Loss
#' 
#' @param X input matrix
#' @param U current guess
#' @param Omega index set
#' @param lambda regularization parameter
#' @export
#' @examples
#' set.seed(12345)
#' n <- 10; p <- 20
#' X <- matrix(rnorm(n*p), n, p)
#' numel <- n*p
#' Omega <- sample(1:numel, floor(0.5*numel), replace=FALSE)
#' U <- matrix(rnorm(n*p), n, p)
#' U[Omega] <- X[Omega]
#' lambda <- 1
#' matrix_completion_loss(X, U, Omega, lambda)
matrix_completion_loss <- function(X, U, Omega, lambda) {
  diff_matrix=project_omega(X,Omega)-project_omega(U,Omega);
  nuclear_norm=sum(svd(U)$d);
  return((norm(diff_matrix,type="F"))^2*0.5+lambda*nuclear_norm)
}
```

**Step 6:** Write a function "prox_grad" that performs proximal gradient descent. Use the Lipschitz constant you derived above to guide your choice of the step size. Note that convergence is still guaranteed with a fixed step size if the step-size is less than twice the reciprical of the Lipschitz constant. Terminate the algorithm once the relative change in the objective falls below a tolerance or if a maximum number of iterations is achieved.

```{r, echo=TRUE}
#' Proximal Gradient Descent
#' 
#' @param X input matrix
#' @param U current guess
#' @param Omega index set
#' @param lambda regularization parameter
#' @param t step-size parameter
#' @param max_iter maximum number of iterations
#' @param tol convergence tolerance
#' @export
#' @examples
#' set.seed(12345)
#' n <- 10; p <- 20
#' X <- matrix(rnorm(n*p), n, p)
#' numel <- n*p
#' Omega <- sample(1:numel, floor(0.5*numel), replace=FALSE)
#' U <- matrix(rnorm(n*p), n, p)
#' U[Omega] <- X[Omega]
#' lambda <- 1
#' pg_sol <- prox_grad(X, U, Omega, lambda,1)
prox_grad <- function(X, U, Omega, lambda, t, max_iter=1e2, tol=1e-3) {
  times=0;
  U_new=prox_step(X,U,Omega,lambda,t);
  obj_new=matrix_completion_loss(X,U_new,Omega,lambda);
  obj=obj_new;
  norm_X_F=norm(X,type="F");
  relative_change_function_value=tol+1;
  while(times<max_iter&(relative_change_function_value>tol)){#|(obj_new<obj)
    times=times+1;
    obj=obj_new;
    U_new=prox_step(X,U_new,Omega,lambda,t);
    obj_new=matrix_completion_loss(X,U_new,Omega,lambda);
    #print(paste0(times," loss: ",obj_new,"change",relative_change_function_value));#
    relative_change_function_value=obj/obj_new-1;
    relative_change_iterate_value=norm(U_new-X,type="F")^2/norm_X_F^2;
  }
  return(list(iterate_value=U_new,
              objective_function_value=matrix_completion_loss(X,U_new,Omega,lambda),
              relative_change_function_value=relative_change_function_value,
              relative_change_iterate_value=relative_change_iterate_value));
  
}

```

Your function should return

- The final iterate value
- The objective function values
- The relative change in the function values
- The relative change in the iterate values

**Step 7:** Write a function "soft_impute" that performs soft-impute algorithm. Terminate the algorithm once the relative change in the objective falls below a tolerance or if a maximum number of iterations is achieved.

```{r, echo=TRUE}
#' Soft-Impute Algorithm
#' 
#' @param X input matrix
#' @param U current guess
#' @param Omega index set
#' @param lambda regularization parameter
#' @param max_iter maximum number of iterations
#' @param tol convergence tolerance
#' @export
#' @examples
#' set.seed(12345)
#' n <- 10; p <- 20
#' X <- matrix(rnorm(n*p), n, p)
#' numel <- n*p
#' Omega <- sample(1:numel, floor(0.5*numel), replace=FALSE)
#' U <- matrix(rnorm(n*p), n, p)
#' U[Omega] <- X[Omega]
#' lambda <- 1
#' si_sol <- soft_impute(X, U, Omega, lambda)
soft_impute <- function(X, U, Omega, lambda, max_iter=1e2, tol=1e-3) {
  times=0;n=nrow(X);p=ncol(X);total_seq=1:(n*p);
  U_new=U;
  Omega_compliment=total_seq[-Omega];
  obj_new=matrix_completion_loss(X,U_new,Omega,lambda);
  norm_X_F=norm(X,"F");
  relative_change_function_value=1;
  relative_change_iterate_value=1;
  obj=obj_new;
  
  while(times<max_iter&(relative_change_function_value>tol|obj_new>obj)){
    times=times+1; 
    obj=obj_new;#renew
    Y_matrix=project_omega(X,Omega)+project_omega(U_new,Omega_compliment);
    decomposition=svd(Y_matrix);
    new_d=softthreshold(decomposition$d,lambda);
    U_new=decomposition$u%*%diag(new_d)%*%t(decomposition$v);
    obj_new=matrix_completion_loss(X,U_new,Omega,lambda);
    #loss=matrix_completion_loss(X,U_new,Omega,lambda);
    relative_change_function_value=obj/obj_new-1;
    relative_change_iterate_value=norm(U_new-X,type="F")^2/norm_X_F^2;
    #print(paste0(times," function ",relative_change_function_value," change_function ",relative_change_function_value));
  }
  return(list(iterate_value=U_new,
              objective_function_value=matrix_completion_loss(X,U_new,Omega,lambda),
              relative_change_function_value=relative_change_function_value,
              relative__change_iterate_value=relative_change_iterate_value));
}
```

Your function should return

- The final iterate value
- The objective function values
- The relative change in the function values
- The relative change in the iterate values

**Step 8:** Apply your two algorithms on images A and B. Use 5 different values of $\lambda$ equally spaced on the logarithmic scale between $10^{-6}$ and $\lambda_{\max}$. Plot a sequence of images corresponding to your recovered matrix estimates. Plot the images in 4-level grayscale. You might find the `gray` function useful.

Use prox_grad for image A
```{r}
imageA <- as.matrix(read.table('imageA.csv', sep=",", header=FALSE))
image(imageA, col=gray(0:3/3))

#find Omega
Omega=which(is.na(imageA)==FALSE)#observed
Not_Omega=which(is.na(imageA))#not observed

n <- nrow(imageA); p <- ncol(imageA)
UA <- matrix(rnorm(n*p), n, p)
UA[Omega] <- imageA[Omega]

lambda=exp(seq(from=10^(-6),to=5,length.out = 5))
XA=imageA
XA[Not_Omega]=0

image(prox_grad(XA, UA, Omega, lambda[1], t=1, max_iter=1e2, tol=1e-3)$iterate_value,col=gray(0:4/4))
image(prox_grad(XA, UA, Omega, lambda[2], t=1, max_iter=1e2, tol=1e-3)$iterate_value,col=gray(0:4/4))
image(prox_grad(XA, UA, Omega, lambda[3], t=1, max_iter=1e2, tol=1e-3)$iterate_value,col=gray(0:4/4))
image(prox_grad(XA, UA, Omega, lambda[4], t=1, max_iter=1e2, tol=1e-3)$iterate_value,col=gray(0:4/4))
image(prox_grad(XA, UA, Omega, lambda[5], t=1, max_iter=1e2, tol=1e-3)$iterate_value,col=gray(0:4/4))

```


Use prox_grad for image B
```{r}
imageB <- as.matrix(read.table('imageB.csv', sep=",", header=FALSE))
image(imageB, col=gray(0:3/3))


#find Omega
Omega=which(is.na(imageB)==FALSE)#observed
Not_Omega=which(is.na(imageB))#not observed

n <- nrow(imageB); p <- ncol(imageB)
UB <- matrix(rnorm(n*p), n, p)
UB[Omega] <- imageB[Omega]
#
lambda=exp(seq(from=10^(-6),to=5,length.out = 5))
XB=imageB
XB[Not_Omega]=0

image(prox_grad(XB, UB, Omega, lambda[1], t=1, max_iter=1e2, tol=1e-3)$iterate_value,col=gray(0:4/4))
image(prox_grad(XB, UB, Omega, lambda[2], t=1, max_iter=1e2, tol=1e-3)$iterate_value,col=gray(0:4/4))
image(prox_grad(XB, UB, Omega, lambda[3], t=1, max_iter=1e2, tol=1e-3)$iterate_value,col=gray(0:4/4))
image(prox_grad(XB, UB, Omega, lambda[4], t=1, max_iter=1e2, tol=1e-3)$iterate_value,col=gray(0:4/4))
image(prox_grad(XB, UB, Omega, lambda[5], t=1, max_iter=1e2, tol=1e-3)$iterate_value,col=gray(0:4/4))


```

Use soft_impute for image A
```{r, echo=TRUE, fig.wight=3, fig.height=3}
imageA <- as.matrix(read.table('imageA.csv', sep=",", header=FALSE))
image(imageA, col=gray(0:3/3))

#find Omega
Omega=which(is.na(imageA)==FALSE)#observed
Not_Omega=which(is.na(imageA))#not observed

n <- nrow(imageA); p <- ncol(imageA)
UA <- matrix(rnorm(n*p), n, p)
UA[Omega] <- imageA[Omega]

lambda=exp(seq(from=10^(-6),to=5,length.out = 5))
XA=imageA
XA[Not_Omega]=0

image(soft_impute(XA, UA, Omega, lambda[1])$iterate_value,col=gray(0:4/4))
image(soft_impute(XA, UA, Omega, lambda[2])$iterate_value,col=gray(0:4/4))
image(soft_impute(XA, UA, Omega, lambda[3])$iterate_value,col=gray(0:4/4))
image(soft_impute(XA, UA, Omega, lambda[4])$iterate_value,col=gray(0:4/4))
image(soft_impute(XA, UA, Omega, lambda[5])$iterate_value,col=gray(0:4/4))

```

Use soft_impute for image B
```{r, echo=TRUE, fig.wight=3, fig.height=3}
imageB <- as.matrix(read.table('imageB.csv', sep=",", header=FALSE))
image(imageB, col=gray(0:3/3))


#find Omega
Omega=which(is.na(imageB)==FALSE)#observed
Not_Omega=which(is.na(imageB))#not observed

n <- nrow(imageB); p <- ncol(imageB)
UB <- matrix(rnorm(n*p), n, p)
UB[Omega] <- imageB[Omega]
#
lambda=exp(seq(from=10^(-6),to=5,length.out = 5))
XB=imageB
XB[Not_Omega]=0

image(soft_impute(XB, UB, Omega, lambda[1])$iterate_value,col=gray(0:4/4))
image(soft_impute(XB, UB, Omega, lambda[2])$iterate_value,col=gray(0:4/4))
image(soft_impute(XB, UB, Omega, lambda[3])$iterate_value,col=gray(0:4/4))
image(soft_impute(XB, UB, Omega, lambda[4])$iterate_value,col=gray(0:4/4))
image(soft_impute(XB, UB, Omega, lambda[5])$iterate_value,col=gray(0:4/4))



```
